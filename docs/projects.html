<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects | Alexander Lori</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <nav>
      <h1>Alexander Lori</h1>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="projects.html">Projects</a></li>
        <li><a href="contact.html">Contact</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <section class="hero">
      <h2>Projects</h2>

      <h3> Steam Review Sentiment & Playtime Modeling</h3>
      <p><strong>Capstone Project (2025)</strong> — Used real-world Steam review data to predict user playtime and analyze engagement trends based on sentiment and user profiles.</p>
      <ul>
        <li>Collected reviews via the Steam API across 3 games and 60,000 reviews from: Baldur’s Gate 3, Darktide, and New World</li>
        <li>Engineered features from review text and user behavior (e.g. lexicon scores, review length, total playtime)</li>
        <li>Clustered 5 distinct user types using k-means with silhouette and SSE validation</li>
        <li>Built regression models to predict playtime; XGBoost achieved R² = 0.707</li>
        <li>Negative reviewers played longer before reviewing and often stopped shortly after</li>
      </ul>

      <h4>Normalized Playtime Behavior</h4>
      <p>To better understand how users interact with games before and after leaving reviews, I created a ratio feature: <code>playtime_at_review / total_playtime</code>. This revealed that users who left negative reviews typically spent a greater portion of their total playtime before submitting their feedback — indicating they gave the game a fair try, then disengaged.</p>
      <img src="playtime_ratio_boxplot.png" alt="Boxplot of playtime ratio by sentiment" width="400" />

      <h4>Model Insights: Who Plays More?</h4>
      <p>Feature importance analysis revealed an unexpected trend: the <strong>number of reviews</strong> on a user's account was the most predictive factor for total playtime — more important than review sentiment or engagement timing. Users with <em>fewer reviews</em> generally spent more time on the game, while frequent reviewers often logged shorter play sessions. This insight suggests that reviewing behavior is a strong proxy for user type and engagement style.</p>

      <img src="mean_playtime_by_reviews.png" alt="Num reviews vs avg playtime" width="400" />
      <h3> IMDb Sentiment Classification & Topic Modeling</h3>
      <p><strong>Class Project (2023)</strong> — Analyzed 50,000 IMDb movie reviews using NLP techniques to explore sentiment, topics, and model performance.</p>
      <ul>
        <li>Preprocessed text and engineered features using CountVectorizer and TF-IDF</li>
        <li>Compared lexicon baseline (73.2% accuracy) with ML models: SVM, Naive Bayes, and a stacked classifier (89.6%)</li>
        <li>Applied topic modeling with LDA to extract dominant themes from review clusters</li>
        <li>Used clustering and word clouds to identify patterns in review content and sentiment</li>
      </ul>
       <h4>Topic Modeling Through Clustering</h4>
      <p>To better understand the types of conversations present in movie reviews, I performed clustering on the TF-IDF vectorized data and visualized each cluster as a word cloud. This revealed five distinct thematic groups among the reviews:</p>
      <ul>
        <li><strong>Cluster 1:</strong> Focused on book-to-film adaptations</li>
        <li><strong>Cluster 2:</strong> Predominantly negative reviews with emphasis on acting and plot issues</li>
        <li><strong>Cluster 3:</strong> Strongly positive reviews with emotionally supportive language</li>
        <li><strong>Cluster 4:</strong> Balanced discussion of themes and storytelling</li>
        <li><strong>Cluster 5:</strong> Discussion of television shows and spin-offs</li>
      </ul>
      <p>These clusters helped validate the topic separation found in LDA modeling, providing a more interpretable view of sentiment and subject matter patterns across the IMDb dataset.</p>

      <img src="kmeansclouds.png" alt="Cluster 1 cloud" width="400" />
      <img src="kmeansclouds2.png" alt="Cluster 2 cloud" width="400" />
      <img src="kmeansclouds3.png" alt="Cluster 3 cloud" width="400" />
      <img src="kmeansclouds4.png" alt="Cluster 4 cloud" width="400" />
      <img src="kmeansclouds5.png" alt="Cluster 5 cloud" width="400" />
      

      <h4>Sentiment Prediction Model Performance</h4>
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Accuracy</th>
            <th>Precision</th>
            <th>Recall</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Lexicon</td>
            <td>73.21%</td>
            <td>72.38%</td>
            <td>75.06%</td>
          </tr>
          <tr>
            <td>SVM</td>
            <td>88.95%</td>
            <td>87.83%</td>
            <td>90.42%</td>
          </tr>
          <tr>
            <td>Random Forest</td>
            <td>82.30%</td>
            <td>79.60%</td>
            <td>86.80%</td>
          </tr>
          <tr>
            <td>Naive Bayes</td>
            <td>88.95%</td>
            <td>87.80%</td>
            <td>90.40%</td>
          </tr>
          <tr>
            <td><strong>Stacked (SVM+NB)</strong></td>
            <td><strong>89.60%</strong></td>
            <td><strong>88.60%</strong></td>
            <td><strong>90.90%</strong></td>
          </tr>
        </tbody>
      </table>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 Alexander Lori. All rights reserved.</p>
  </footer>
</body>
</html>
